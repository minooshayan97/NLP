{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heartdisease corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile=open('heartdisease.txt')\n",
    "text = txtfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persian text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer\n",
    "normalizer = Normalizer()\n",
    "text_norm = []\n",
    "for sent in text:\n",
    "    sent_norm = normalizer.normalize(sent)\n",
    "    text_norm.append(sent_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "sentences = []\n",
    "for sent in text_norm:\n",
    "    sent_norm = tokenizer.tokenize_sentences(sent)\n",
    "    sentences+=sent_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenized_sentences= []\n",
    "for sent in sentences:\n",
    "    sent_norm = tokenizer.tokenize_words(sent)\n",
    "    tokenized_sentences.append(sent_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "word_counter = 0\n",
    "for sent in tokenized_sentences:\n",
    "    for word in sent:\n",
    "        word_counter += 1\n",
    "        if word in punctuation:\n",
    "            sent.remove(word)\n",
    "            word_counter -= 1\n",
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split into train and test sets of size >= 20000 and 2000 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(words):\n",
    "    word_counter = 0\n",
    "    for sent in words:\n",
    "        for word in sent:\n",
    "            word_counter += 1\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_size = 200\n",
    "test_text = random.sample(tokenized_sentences, test_size)\n",
    "train_text = []\n",
    "\n",
    "for sent in tokenized_sentences:\n",
    "    if sent not in test_text:\n",
    "        train_text.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word count :\n",
      "31271\n",
      "test word count :\n",
      "2458\n"
     ]
    }
   ],
   "source": [
    "train_count = counter(train_text)\n",
    "test_count = counter(test_text)\n",
    "\n",
    "print(\"train word count :\")\n",
    "print(train_count)\n",
    "print(\"test word count :\")\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['واژه', 'بیماری', 'قلبی', 'معمولا', 'برای', 'اشاره', 'به', 'حمله', 'قلبی', 'اشاره', 'می\\u200cشود', '،', 'با', 'اینحال', 'بیماری', 'قلبی', 'شامل', 'سایر', 'بیماری\\u200cهای', 'قلب', 'از', 'جمله', 'بیماری', 'عروق', 'کرونر', '،', 'نارسایی', 'قلبی', '،', 'سکته', 'قلبی', '،', 'آریتمی', 'قلبی', 'و', 'کاردیومیوپاتی', 'می\\u200cباشد']\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModel(n,test_data,train_text):\n",
    "    model = MLE(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    #train_unigrams = everygrams(flatten(pad_both_ends(sent, n=1) for sent in train_text), max_len=1)\n",
    "\n",
    "    model.fit(train, vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "test_padded = list(flatten(pad_both_ends(sent, n=1) for sent in test_text))\n",
    "test_unigrams = list(everygrams(test_padded, max_len=1))\n",
    "\n",
    "model1 = n_gramModel(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "test_padded = list(flatten(pad_both_ends(sent, n=2) for sent in test_text))\n",
    "test_bigrams = list(bigrams(test_padded))\n",
    "\n",
    "model2 = n_gramModel(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "test_padded = list(flatten(pad_both_ends(sent, n=3) for sent in test_text))\n",
    "test_trigrams = list(trigrams(test_padded))\n",
    "\n",
    "model3 = n_gramModel(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : Lidstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Lidstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelLidstone(n,test_data,train_text):\n",
    "    model = Lidstone(n,0.01)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "656.3968742108017\n",
      "model entropy on test sentences :\n",
      "9.358424558468958\n"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelLidstone(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "1552.8289630599802\n",
      "model entropy on test sentences :\n",
      "10.60068321694101\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelLidstone(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "2714.131278852394\n",
      "model entropy on test sentences :\n",
      "11.40627478831084\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelLidstone(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : add-one (Laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelLaplace(n,test_data,train_text):\n",
    "    model = Laplace(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "656.3968742108017\n",
      "model entropy on test sentences :\n",
      "9.358424558468958\n"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelLaplace(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "1219.9018900413228\n",
      "model entropy on test sentences :\n",
      "10.25254940913677\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelLaplace(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "2302.051541073207\n",
      "model entropy on test sentences :\n",
      "11.168704419269634\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelLaplace(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : KneserNey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelKneserNey(n,test_data,train_text):\n",
    "    model = KneserNeyInterpolated(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-957f74c37f6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_gramModelKneserNey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_unigrams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-505a1e29ad0e>\u001b[0m in \u001b[0;36mn_gramModelKneserNey\u001b[1;34m(n, test_data, train_text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model perplexity on test sentences :\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model entropy on test sentences :\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mperplexity\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m    163\u001b[0m         return -1 * _mean(\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m    163\u001b[0m         return -1 * _mean(\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mlogscore\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_base2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcontext_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \"\"\"\n\u001b[1;32m--> 117\u001b[1;33m         return self.unmasked_score(\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\models.py\u001b[0m in \u001b[0;36munmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# The base recursion case: no context, we only have a unigram.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munigram_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;31m# It can also happen that we have no data for this context.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\lm\\smoothing.py\u001b[0m in \u001b[0;36munigram_score\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munigram_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mword_continuation_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_continuation_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mword_continuation_count\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0malpha_gamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelKneserNey(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelKneserNey(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelKneserNey(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : WittenBell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import WittenBellInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelWittenBell(n,test_data,train_text):\n",
    "    model = WittenBellInterpolated(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelWittenBell(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelWittenBell(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelWittenBell(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : backoff (stupid backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import StupidBackoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelbackoff(n,test_data,train_text):\n",
    "    model = StupidBackoff(n,0.4)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelbackoff(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelbackoff(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelbackoff(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing method : AbsoluteDiscounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import AbsoluteDiscountingInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramModelAbsoluteDisc(n,test_data,train_text):\n",
    "    model = AbsoluteDiscountingInterpolated(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, train_text)\n",
    "    model.fit(train,vocab)\n",
    "    \n",
    "    print(\"model vocabulary :\")\n",
    "    print(model.vocab)\n",
    "    print(len(model.vocab))\n",
    "    \n",
    "    print(\"model perplexity on test sentences :\")\n",
    "    print(model.perplexity(test_data))\n",
    "    print(\"model entropy on test sentences :\")\n",
    "    print(model.entropy(test_data))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3614 items>\n",
      "3614\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model1 = n_gramModelAbsoluteDisc(1,test_unigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model2 = n_gramModelAbsoluteDisc(2,test_bigrams,train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocabulary :\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3616 items>\n",
      "3616\n",
      "model perplexity on test sentences :\n",
      "inf\n",
      "model entropy on test sentences :\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "model3 = n_gramModelAbsoluteDisc(3,test_trigrams,train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
